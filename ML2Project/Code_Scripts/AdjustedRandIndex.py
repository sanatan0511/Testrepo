import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.impute import SimpleImputer
from sklearn.metrics import adjusted_rand_score
import numpy as np

# --- 1. FULL DATA PIPELINE (Reloading and Imputing) ---
# This ensures we have the necessary input file for clustering

print("1. Re-running complete data pipeline...")

# Load Raw Data
df_census = pd.read_csv("Raw_Data/CombinedData(census).csv")
df_cities_states = pd.read_csv("Raw_Data/Indian_Cities_States(Sheet1).csv")
df_residex = pd.read_csv("Raw_Data/CombinedData(Residex_Data).csv")
df_cpi = pd.read_csv("Raw_Data/CombinedData(Consumer Price Index).csv")
df_income = pd.read_csv("Raw_Data/Per_Capita_Income_1763716755692.xlsx - Per_Capita_Income.csv")

# Filter and Clean Income Data
latest_year = df_income[df_income['Price Category'] == 'current']['Year'].max()
df_income_latest = df_income[(df_income['Price Category'] == 'current') & (df_income['Year'] == latest_year)].copy()
df_income_latest.rename(columns={'Price (in â‚¹)': 'Per_Capita_Income_Latest_Current'}, inplace=True)
df_income_latest = df_income_latest[['State', 'Per_Capita_Income_Latest_Current']]

# Standardization Function
def standardize_state(state):
    if pd.isna(state): return state
    state = str(state).upper().replace(' & ', ' AND ').replace('.', '').replace(' (TRICITY)', '').strip()
    if 'ANDAMAN' in state: return 'ANDAMAN AND NICOBAR ISLANDS'
    if 'CHANDIGARH' in state: return 'CHANDIGARH'
    if 'DELHI' in state: return 'DELHI'
    if 'UTTAR PRADESH' in state: return 'UTTAR PRADESH'
    if 'WEST BENGAL' in state: return 'WEST BENGAL'
    return state

# Apply Cleaning
df_census['UA_Name_Clean'] = df_census['UA Name'].str.replace(r' \(.*\)', '', regex=True).str.strip()
df_residex.rename(columns={'City': 'UA_Name_Clean'}, inplace=True)
df_residex['UA_Name_Clean'] = df_residex['UA_Name_Clean'].str.replace(r' \(.*\)', '', regex=True).str.strip()
df_residex = df_residex[['UA_Name_Clean', 'Latest HPI', 'yoy_change']] 
df_cities_states.rename(columns={'State / Union Territory': 'State_Standard'}, inplace=True)
df_cities_states['State_Standard'] = df_cities_states['State_Standard'].apply(standardize_state)
df_income_latest.rename(columns={'State': 'State_Standard'}, inplace=True)
df_income_latest['State_Standard'] = df_income_latest['State_Standard'].apply(standardize_state)
df_cpi.rename(columns={'State / Union Teritory': 'State_Standard'}, inplace=True)
df_cpi['State_Standard'] = df_cpi['State_Standard'].apply(standardize_state)
df_cpi.columns = df_cpi.columns.str.strip()

# Merges
df_merged = pd.merge(df_cities_states, df_census, left_on='City', right_on='UA_Name_Clean', how='left')
df_merged = pd.merge(df_merged, df_residex, on='UA_Name_Clean', how='left')
df_merged = pd.merge(df_merged, df_income_latest, on='State_Standard', how='left')
df_merged = pd.merge(df_merged, df_cpi, on='State_Standard', how='left')

# Feature Engineering
df_merged['Latest HPI'] = pd.to_numeric(df_merged['Latest HPI'], errors='coerce')
df_merged['Per_Capita_Income_Latest_Current'] = pd.to_numeric(df_merged['Per_Capita_Income_Latest_Current'], errors='coerce')
df_merged['Price_to_Income_Ratio'] = (df_merged['Latest HPI'] / (df_merged['Per_Capita_Income_Latest_Current'] / 100000))
df_merged['yoy_change'] = pd.to_numeric(df_merged['yoy_change'], errors='coerce')
df_merged['Affordability_Change_Pressure'] = df_merged['Latest HPI'] * (df_merged['yoy_change'] / 100)

clustering_features = [
    'City', 'State_Standard', 'Latest HPI', 'yoy_change', 'CPI_Housing_Index', 'P_LIT', 'TOT_WORK_P',
    'Literacy_rate', 'Worker Participation Rate', 'SC/ST Population Percentage', 'Households per Capita',
    'Per_Capita_Income_Latest_Current', 'Price_to_Income_Ratio', 'Affordability_Change_Pressure'
]
df_final = df_merged[clustering_features].copy()
numerical_cols = df_final.select_dtypes(include=np.number).columns.tolist()

# Imputation
imputer = SimpleImputer(strategy='median') 
df_final[numerical_cols] = imputer.fit_transform(df_final[numerical_cols])
df_imputed = df_final.copy() # Final imputed data

# --- 2. CLUSTERING AND COMPARISON ---
print("2. Running K-Means and DBSCAN for comparison...")

# Setup data for clustering
feature_cols = df_imputed.select_dtypes(include=np.number).columns.tolist()
X = df_imputed[feature_cols]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# A. K-Means (K=3)
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42, n_init=10)
df_imputed['KM_Cluster'] = kmeans.fit_predict(X_scaled)

# B. DBSCAN (Parameters used previously)
dbscan = DBSCAN(eps=0.5, min_samples=5) 
df_imputed['DBSCAN_Cluster'] = dbscan.fit_predict(X_scaled)

# --- 3. ADJUSTED RAND INDEX (ARI) CALCULATION ---

# ARI 1: All Points (K-Means vs. DBSCAN, including Noise=-1)
ari_all = adjusted_rand_score(df_imputed['KM_Cluster'], df_imputed['DBSCAN_Cluster'])

# ARI 2: Clustered Points ONLY (Excluding DBSCAN Noise)
non_noise_indices = df_imputed['DBSCAN_Cluster'] != -1
df_filtered = df_imputed[non_noise_indices].copy()

# ARI is calculated on the subset of data points that DBSCAN successfully clustered
ari_clustered = adjusted_rand_score(df_filtered['KM_Cluster'], df_filtered['DBSCAN_Cluster'])


# --- 4. FINAL OUTPUT ---
print("\n--- Final Quantitative Comparison (K-Means vs. DBSCAN) ---")
print(f"1. ARI (All Points, including DBSCAN Noise, n=50): {ari_all:.4f}")
print(f"2. ARI (Clustered Points ONLY, n={len(df_filtered)}): {ari_clustered:.4f}")
print("\nConclusion for Paper: The scores near 0.0 confirm that the two models solve the clustering problem differently, validating the choice of K-Means for policy structure.")